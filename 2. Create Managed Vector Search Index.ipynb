{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9499b2-7001-49a4-92ec-291d89f06d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Create Managed Vector Search Index\n",
    "\n",
    "The process of creating a **managed** Vector Search index for retrieval-augmented generation (RAG) applications. This involves configuring Databricks Vector Search to ingest data from a Delta table containing text embeddings and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d976e8c8-b5b9-433d-9436-334f0d35e098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq databricks-vectorsearch databricks-sdk flashrank PyPDF2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750b6417-1455-40df-8074-a8ed5a657dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c33d9b7-e23d-4182-a8fa-3c7733e1f4f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3f49ae-f775-4675-829e-d9d006b95370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser10914379_1753166678@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser10914379_1753166678\nWorking Directory: /Volumes/dbacademy/ops/labuser10914379_1753166678@vocareum_com\nDataset Location:  NestedNamespace (arxiv='/Volumes/dbacademy_arxiv/v01', dais='/Volumes/dbacademy_dais/v01', news='/Volumes/dbacademy_news/v01', docs='/Volumes/dbacademy_docs/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0021e987-e56d-4565-b36c-1de8446e6ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create a Vector Search Endpoint\n",
    "\n",
    "To start, you need to create a Vector Search endpoint to serve the index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6792321-9761-469a-91ca-92d7b2279849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step-by-Step Instructions:\n",
    "\n",
    "\n",
    "**Vector Search Endpoint**: The first step for creating a Vector Search index is to create a compute endpoint. This endpoint is already created in this lab environment.\n",
    "\n",
    "**Wait for Endpoint to be Ready**: After defining the endpoint name, check the status of the endpoint using the provided function `wait_for_vs_endpoint_to_be_ready`.\n",
    "\n",
    "Additionally, you can check the endpoint status in the Databricks workspace [Vector Search Endpoints in Compute section](#/setting/clusters/vector-search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d29e9d0-8aae-4f65-86e0-b40927bf47dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Vector Search endpoint name: vs_endpoint_4.\n"
     ]
    }
   ],
   "source": [
    "## assign vs search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "vs_endpoint_name = vs_endpoint_prefix + str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Assigned Vector Search endpoint name: {vs_endpoint_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f8bedc-3f9c-4e48-965b-2c29577df244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint named vs_endpoint_4 is ready.\n"
     ]
    }
   ],
   "source": [
    "import databricks.sdk.service.catalog as c\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "## check the status of the endpoint.\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name)\n",
    "print(f\"Endpoint named {vs_endpoint_name} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa6598e-fe10-4f0c-88bf-2e0937996457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create a Managed Vector Search Index\n",
    "\n",
    "Now, connect the Delta table containing text and metadata with the Vector Search endpoint. In this , you will create a **managed** index, which means you don't need to create the embeddings manually. For API details, check the [documentation page](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-index-using-the-python-sdk).\n",
    "\n",
    "\n",
    "**\uD83D\uDCCC Note 1: You will use the embeddings table that you created in the previous lab. If you haven't completed that lab, stop here and complete it first.**\n",
    "\n",
    "**\uD83D\uDCCC Note 2:** Although the source table already has the embedding column precomputed, we are not going to use it here to test the managed vector search capability to populate embeddings on the fly during data ingestion and query.\n",
    "\n",
    "**\uD83D\uDCA1 Instructions:**\n",
    "\n",
    "1. Define the source Delta table containing the text to be indexed.\n",
    "\n",
    "1. Create a Vector Search index. Use these parameters; source column as `content` and `databricks-gte-large-en` as embedding model. Also, the sync process should be  `manually triggered`.\n",
    "\n",
    "1. Create or synchronize the Vector Search index based on the source Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23846dd0-f0b3-484c-8190-2b4c8906ea16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "ALTER TABLE dbacademy.labuser10914379_1753166678.pdf_text_embeddings\n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ba6986-2452-4f70-845a-e4978ed00847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'id' column already present in the table.\nIndex 'dbacademy.labuser10914379_1753166678.pdf_text_managed_vs_index' already exists. Triggering sync...\nWaiting for the index to be ready...\nIndex is ready.\n"
     ]
    }
   ],
   "source": [
    "# Define full table names\n",
    "source_table_fullname = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_embeddings\"\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_managed_vs_index\"\n",
    "\n",
    "# Optional: Ensure the source table has an `id` column\n",
    "# You can skip this part if you've already added it manually\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = spark.table(source_table_fullname)\n",
    "if 'id' not in df.columns:\n",
    "    print(\"Adding 'id' column to the source table...\")\n",
    "    df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(source_table_fullname)\n",
    "else:\n",
    "    print(\"'id' column already present in the table.\")\n",
    "\n",
    "# Create or sync the index\n",
    "if not index_exists(vsc, vs_endpoint_name, vs_index_fullname):\n",
    "    print(f\"Creating index '{vs_index_fullname}' on endpoint '{vs_endpoint_name}'...\")\n",
    "    \n",
    "    vsc.create_delta_sync_index(\n",
    "        endpoint_name=vs_endpoint_name,\n",
    "        index_name=vs_index_fullname,\n",
    "        source_table_name=source_table_fullname,\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        primary_key=\"id\",\n",
    "        embedding_source_column=\"content\",  # The column containing raw text\n",
    "        embedding_model_endpoint_name=\"databricks-gte-large-en\"  # Pretrained embedding model\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index '{vs_index_fullname}' already exists. Triggering sync...\")\n",
    "    vsc.get_index(vs_endpoint_name, vs_index_fullname).sync()\n",
    "\n",
    "# Wait for the index to be ready\n",
    "print(\"Waiting for the index to be ready...\")\n",
    "wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)\n",
    "print(\"Index is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7537401b-5675-424c-8e88-1e89ca745518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Search Documents Similar to the Query\n",
    "\n",
    "Test the Vector Search index by searching for similar content based on a sample query.\n",
    "\n",
    "**\uD83D\uDCA1 Instructions:**\n",
    "\n",
    "1. Get the index instance that we created.\n",
    "\n",
    "1. Send a sample query to the language model endpoint using **query text**. \uD83D\uDEA8 Note: As you created a managed index, you will use plain text for similarity search using `query_text` parameter.\n",
    "\n",
    "1. Use the embeddings to search for similar content in the Vector Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f9c91e-a044-449e-ab20-6e40b3a6437c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n[['dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'sonally identiﬁable information, and Internet Relay Chat (IRC) conversations. Even worse, because of the\\nbillion parameters of large models, it is easy for PFM to learn private information, making the larger model\\nmore vulnerable than smaller models. We must take privacy-preserving measures into account during all\\nPFM processes, including data processing, model training, model inference, and system deployment, in\\norder to reduce the risks of privacy leakage.', 0.0029254896], ['dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'order to reduce the risks of privacy leakage.\\n8 Future Research Challenges and Open Problems\\nThe PFM can avoid training models from the scratch, which is a breakthrough from weak AI to general AI.\\nAt present, due to the characteristics of PFM such as large-scale parameters, a large amount of training data,\\n39\\nand high computational complexity, there are still many technical challenges in PFMs. We summarize the', 0.0027920706], ['dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'the modiﬁed image, which is the original purpose of the adversarial sample. Some work has found that pre-\\ntrained LMs are vulnerable in some scenarios. Jin et al. [262] successfully attack the three target models of\\nBERT, CNN, and RNN by generating natural adversarial samples, which indicates that the current language\\nprocessing model still has a large room for improvement in terms of security. However, it is difﬁcult to', 0.0027121648], ['dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'reduce defect inheritance during transfer learning while retaining useful knowledge from the PFM.\\nData Privacy in PFMs LLMs and other PFMs have been trained on private datasets [270]. The re-\\nsearchers have discovered that by querying the massive LMs, it is feasible to recover speciﬁc training sam-\\nples. An adversary may, for instance, obtain IRC discussions and personally identiﬁable information. Even', 0.002578651]]\n"
     ]
    }
   ],
   "source": [
    "## get VS index\n",
    "index = vsc.get_index(vs_endpoint_name, vs_index_fullname)\n",
    "\n",
    "question = \"What are the security and privacy concerns when training generative models?\"\n",
    "\n",
    "## search for similar documents  \n",
    "results = index.similarity_search(\n",
    "    query_text = question,\n",
    "    columns=[\"pdf_name\", \"content\"],\n",
    "    num_results=4\n",
    "    )\n",
    "\n",
    "## show the results\n",
    "docs = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1175e2e9-2d53-4d24-86cc-d632cc761507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Re-rank Search Results\n",
    "\n",
    "You have retrieved some documents that are similar to the query text. However, the question of which documents are the most relevant is not done by the vector search results. Use `flashrank` library to re-rank the results and show the most relevant top 3 documents. \n",
    "\n",
    "**\uD83D\uDCA1 Instructions:**\n",
    "\n",
    "1. Define `flashrank` with **`rank-T5-flan`** model.\n",
    "\n",
    "1. Re-rank the search results.\n",
    "\n",
    "1. Show the most relevant **top 3** documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe272bda-591c-4a84-b21e-64de15fb344c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'text': 'the modiﬁed image, which is the original purpose of the adversarial sample. Some work has found that pre-\\ntrained LMs are vulnerable in some scenarios. Jin et al. [262] successfully attack the three target models of\\nBERT, CNN, and RNN by generating natural adversarial samples, which indicates that the current language\\nprocessing model still has a large room for improvement in terms of security. However, it is difﬁcult to', 'score': 0.5440121}\n\n{'file': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'text': 'order to reduce the risks of privacy leakage.\\n8 Future Research Challenges and Open Problems\\nThe PFM can avoid training models from the scratch, which is a breakthrough from weak AI to general AI.\\nAt present, due to the characteristics of PFM such as large-scale parameters, a large amount of training data,\\n39\\nand high computational complexity, there are still many technical challenges in PFMs. We summarize the', 'score': 0.531311}\n\n{'file': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf', 'text': 'reduce defect inheritance during transfer learning while retaining useful knowledge from the PFM.\\nData Privacy in PFMs LLMs and other PFMs have been trained on private datasets [270]. The re-\\nsearchers have discovered that by querying the massive LMs, it is feasible to recover speciﬁc training sam-\\nples. An adversary may, for instance, obtain IRC discussions and personally identiﬁable information. Even', 'score': 0.5243332}\n"
     ]
    }
   ],
   "source": [
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "## define the ranker.\n",
    "cache_dir = f\"{DA.paths.working_dir}/opt\"\n",
    "\n",
    "ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=cache_dir)\n",
    "\n",
    "## format the result to align with reranker library format. \n",
    "passages = []\n",
    "for doc in docs:\n",
    "    new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "    passages.append(new_doc)\n",
    "\n",
    "## rerank the passages.\n",
    "rerankrequest = RerankRequest(query=question, passages=passages)\n",
    "ranked_passages = ranker.rerank(rerankrequest)\n",
    "\n",
    "## show the top 3 results.\n",
    "print(*ranked_passages[:3], sep=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5326794719916375,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Create Managed Vector Search Index",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}