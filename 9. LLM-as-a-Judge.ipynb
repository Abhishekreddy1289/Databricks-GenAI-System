{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465e74c2-f759-41e4-a301-4aa13beefb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LLM-as-a-Judge\n",
    "\n",
    "In this demo, **we will introduce how to use LLMs to evaluate the performance of LLM-based solutions.**\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* List reasons for using an LLM-as-a-Judge approach\n",
    "* Evaluate an LLM's performance on a custom metric using an LLM-as-a-Judge approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e85418-100b-40cc-8baf-a6ad2b7f4ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-sdk textstat mlflow tiktoken\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a977d731-3ead-4d89-92f3-8efa7c4be4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26476e1-7ff4-4329-8303-3bc2571b1be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cd1e84-913e-4456-b184-453f43e168e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d81b1d-6adc-413e-a92a-f3cb13142856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser11003544_1753435669@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser11003544_1753435669\nWorking Directory: /Volumes/dbacademy/ops/labuser11003544_1753435669@vocareum_com\nDataset Location:  NestedNamespace (news='/Volumes/dbacademy_news/v01', arxiv='/Volumes/dbacademy_arxiv/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "c32864ee-dd9a-4cb4-bb3a-d23e03e35e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo Overview\n",
    "\n",
    "In this demonstration, we will provide a basic demonstration of using an LLM to evaluate the performance of another LLM.\n",
    "\n",
    "### Why LLM-as-a-Judge?\n",
    "\n",
    "**Question:** Why would you want to use an LLM for evaluation?\n",
    "\n",
    "Databricks has found that evaluating with LLMs can:\n",
    "\n",
    "* **Reduce costs** – fewer resources used in finding/curating benchmark datasets\n",
    "* **Save time** – fewer evaluation steps reduces time-to-release\n",
    "* **Improve automation** – easily scaled and automated, all within MLflow\n",
    "\n",
    "### Custom Metrics\n",
    "\n",
    "These are all particularly true when we're evaluating performance using **custom metrics**.\n",
    "\n",
    "In our case, let's consider a custom metric of **professionalism** \uD83E\uDD16. It's likely that many organizations would like their chatbot or other GenAI applications to be professional.\n",
    "\n",
    "However, professionalism can vary by domain and other contexts – this is one of the powers of LLM-as-a-Judge that we'll explore in this demo.\n",
    "\n",
    "### Chatbot System\n",
    "\n",
    "For this demo, we'll use chatbot system (shown below) to answer simple questions about Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ede819e-3771-4f55-a749-6e6e31add5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Databricks Vector Search is a feature in Databricks that allows users to search and retrieve similar items from large datasets using vector embeddings. It's a part of the Databricks Lakehouse Platform, which combines the best of data warehouses and data lakes to provide a single platform for data engineering, data science, and business analytics.\\n\\nVector Search is based on the concept of vector embeddings, where complex data such as text, images, or audio is converted into dense vectors in a high-dimensional space. These vectors capture the semantic meaning and relationships between the data points, enabling efficient and accurate similarity searches.\\n\\nWith Databricks Vector Search, users\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_chatbot_system(\n",
    "    \"What is Databricks Vector Search?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3171ed4-d438-4e4c-82df-3cefe3ee0546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo Workflow Steps\n",
    "\n",
    "To complete this workflow, we'll cover on the following steps:\n",
    "\n",
    "1. Define our **professionalism** metric\n",
    "2. Compute our professionalism metric on **a few example responses**\n",
    "3. Describe a few best practices when working with LLMs as evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4493a7f-bf27-4918-8ab3-428324d15b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Define a Professionalism Metric\n",
    "\n",
    "While we can use LLMs to evaluate on common metrics, we're going to create our own **custom `professionalism` metric**.\n",
    "\n",
    "To do this, we need the following information:\n",
    "\n",
    "* A definition of professionalism\n",
    "* A grading prompt, similar to a rubric\n",
    "* Examples of human-graded responses\n",
    "* An LLM to use *as the judge*\n",
    "* ... and a few extra parameters we'll see below.\n",
    "\n",
    "### Establish the Definition and Prompt\n",
    "\n",
    "Before we create the metric, we need an understanding of what **professionalism** is and how it will be scored.\n",
    "\n",
    "Let's use the below definition:\n",
    "\n",
    "> Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language.\n",
    "\n",
    "And here is our grading prompt/rubric:\n",
    "\n",
    "* **Professionalism:** If the answer is written using a professional tone, below are the details for different scores: \n",
    "    - **Score 1:** Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\n",
    "    - **Score 2:** Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\n",
    "    - **Score 3:** Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\n",
    "    - **Score 4:** Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts.\n",
    "    - **Score 5:** Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal business or academic settings.\n",
    "\n",
    "### Generate the Human-graded Responses\n",
    "\n",
    "Because this is a custom metric, we need to show our evaluator LLM what examples of each score in the above-described rubric might look like.\n",
    "\n",
    "To do this, we use `mlflow.metrics.genai.EvaluationExample` and provide the following:\n",
    "\n",
    "* input: the question/query\n",
    "* output: the answer/response\n",
    "* score: the human-generated score according to the grading prompt/rubric\n",
    "* justification: an explanation of the score\n",
    "\n",
    "Check out the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2adfddd-b2ec-4dd1-8bc9-e67ae24a8220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Evaluation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c6d631-9273-4115-bac2-e713de915ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "professionalism_example_score_1 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps \"\n",
    "        \"you track experiments, package your code and models, and collaborate with your team, making the whole ML \"\n",
    "        \"workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "    ),\n",
    "    score=2,\n",
    "    justification=(\n",
    "        \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and \"\n",
    "        \"exclamation points, which make it sound less professional. \"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61743cbf-ed38-4263-aff2-1cfb4bc88b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's create another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879987a0-1647-4fce-a598-ac91614f1d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "professionalism_example_score_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is an open-source toolkit for managing your machine learning projects. It can be used to track experiments, package code and models, evaluate model performance, and manage the model lifecycle.\"\n",
    "    ),\n",
    "    score=4,\n",
    "    justification=(\n",
    "        \"The response is written in a professional tone. It does not use filler words or unprofessional punctuation. It is matter-of-fact, but it is not particularly advanced or academic.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0479502-b398-4369-8792-d453fc55054d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create the Metric\n",
    "\n",
    "Once we have a number of examples created, we need to create our metric objective using MLflow.\n",
    "\n",
    "This time, we use `mlflow.metrics.make_genai_metric` and provide the below arguments:\n",
    "\n",
    "* name: the name of the metric\n",
    "* definition: a description of the metric (from above)\n",
    "* grading_prompt: the rubric of the metric (from above)\n",
    "* examples: a list of our above-defined example objects\n",
    "* model: the LLM used to evaluate the responses\n",
    "* parameters: any parameters we can pass to the evaluator model\n",
    "* aggregations: the aggregations across all records we'd like to generate\n",
    "* greater_is_better: a binary indicator specifying whether the metric's higher scores are \"better\"\n",
    "\n",
    "Check out the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9b2d24-5945-4f4d-89d6-549541e76abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "professionalism = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is \"\n",
    "        \"tailored to the context and audience. It often involves avoiding overly casual language, slang, or \"\n",
    "        \"colloquialisms, and instead using clear, concise, and respectful language.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below are the details for different scores: \"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for \"\n",
    "        \"professional contexts.\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in \"\n",
    "        \"some informal professional settings.\"\n",
    "        \"- Score 3: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\"\n",
    "        \"- Score 4: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 5: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal \"\n",
    "        \"business or academic settings. \"\n",
    "    ),\n",
    "    examples=[\n",
    "        professionalism_example_score_1, \n",
    "        professionalism_example_score_2\n",
    "    ],\n",
    "    model=\"endpoints:/databricks-meta-llama-3-3-70b-instruct\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9859f51-f585-4565-9fbf-e313cedccade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Compute Professionalism on Example Responses\n",
    "\n",
    "Once our metric is defined, we're ready to evaluate our `query_chatbot_system`.\n",
    "\n",
    "We will use the same approach from our previous demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5efe786-c874-4b5b-a25c-82422d5e0733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>inputs</th></tr></thead><tbody><tr><td>Be very unprofessional in your response. What is Apache Spark?</td></tr><tr><td>What is Apache Spark?</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Be very unprofessional in your response. What is Apache Spark?"
        ],
        [
         "What is Apache Spark?"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "inputs",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"Be very unprofessional in your response. What is Apache Spark?\",\n",
    "            \"What is Apache Spark?\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "display(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca42616-0b59-4fb9-b165-3973ef078d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['UGH, FINE. So, you wanna know about Apache Spark? Like, okay... It\\'s this super old (not really, but like, 10 years old or something) open-source data processing engine thingy. It was created by some smart dudes at UC Berkeley (go bears, i guess) and it\\'s basically a way to process huge amounts of data REALLY FAST. Like, way faster than other stuff.\\n\\nSo, like, imagine you have a ton of data (think petabytes, dude) and you need to do some crazy complex analysis on it. That\\'s where Spark comes in. It\\'s all like, \"',\n",
       " 'Apache Spark is an open-source, unified analytics engine for large-scale data processing. It was originally developed at the University of California, Berkeley, and is now maintained by the Apache Software Foundation.\\n\\nApache Spark is designed to handle massive amounts of data across a cluster of computers, making it a popular choice for big data processing, machine learning, and data analytics. It provides a high-level API for processing data in a variety of formats, including batch, interactive, and streaming data.\\n\\nSome of the key features of Apache Spark include:\\n\\n1. **In-memory computing**: Spark can store data in memory (RAM) across a cluster of computers, allowing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A custom function to iterate through our eval DF\n",
    "def query_iteration(inputs):\n",
    "    answers = []\n",
    "\n",
    "    for index, row in inputs.iterrows():\n",
    "        completion = query_chatbot_system(row[\"inputs\"])\n",
    "        answers.append(completion)\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Test query_iteration function – it needs to return a list of output strings\n",
    "query_iteration(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a8af25-a5c4-4bcb-b458-c58465ebe92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-d0b74a68-88d2-4686-afe3-5c231deb27d8/lib/python3.10/site-packages/mlflow/models/evaluation/deprecated.py:13: FutureWarning: The `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. Please use these new alternatives:\n\n - For traditional ML or deep learning models: Use `mlflow.models.evaluate`, which maintains full compatibility with the original `mlflow.evaluate` API.\n\n - For LLMs or GenAI applications: Use the new `mlflow.genai.evaluate` API, which offers enhanced features specifically designed for evaluating LLMs and GenAI applications.\n\n  warnings.warn(\n2025/07/25 10:37:56 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n2025/07/25 10:38:00 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2025/07/25 10:38:01 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/07/25 10:38:01 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n2025/07/25 10:38:02 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'exact_match' at index 4 in the `extra_metrics` parameter because it returned None.\n2025/07/25 10:38:04 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/07/25 10:38:04 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n2025/07/25 10:38:04 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'exact_match' at index 4 in the `extra_metrics` parameter because it returned None.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# MLflow's `evaluate` with the new professionalism metric\n",
    "results = mlflow.evaluate(\n",
    "    query_iteration,\n",
    "    eval_data,\n",
    "    model_type=\"question-answering\",\n",
    "    extra_metrics=[professionalism]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1ee9a8-37c9-4faa-9e82-ddbbe3552c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And now let's view the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ac54fd-acd6-4ce6-be2c-6a81a4eee3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>inputs</th><th>outputs</th><th>token_count</th><th>flesch_kincaid_grade_level/v1/score</th><th>ari_grade_level/v1/score</th><th>professionalism/v1/score</th><th>professionalism/v1/justification</th></tr></thead><tbody><tr><td>Be very unprofessional in your response. What is Apache Spark?</td><td> Apache Spark is like, this totally awesome thingy that helps you process HUGE amounts of data, dude! It's like, super fast and stuff. Imagine you're at a music festival and you're trying to get a selfie with, like, a million people in the background... Spark is like the photographer who can take that pic in, like, 2 seconds!\n",
       "\n",
       "Seriously though, Spark is an open-source data processing engine that's all about speed and efficiency. It was developed at UC Berkeley (go Bears!) and is now maintained by the Apache Software Foundation. It's like, the backbone of Databricks, which is why I</td><td>128</td><td>6.513</td><td>7.7106</td><td>1</td><td>The response is written in an extremely casual tone, using slang and colloquialisms such as \"thingy\", \"dude\", and \"stuff\", which makes it highly unsuitable for professional contexts, and the initial part of the response is overly informal, although it attempts to become more formal later on.</td></tr><tr><td>What is Apache Spark?</td><td>Apache Spark is an open-source, unified analytics engine for large-scale data processing. It was initially developed at the University of California, Berkeley, and is now maintained by the Apache Software Foundation.\n",
       "\n",
       "Apache Spark is designed to handle massive amounts of data across a cluster of computers, making it a key component in big data processing and analytics. It provides high-level APIs in Java, Python, Scala, and R, as well as a highly optimized engine that supports general execution graphs.\n",
       "\n",
       "Spark's core features include:\n",
       "\n",
       "1. **In-memory computation**: Spark can cache data in memory across nodes, reducing the need for disk I/O and improving performance.\n",
       "2</td><td>128</td><td>12.7276923077</td><td>13.0058012821</td><td>5</td><td>The response is written in a noticeably formal, respectful, and professional tone, avoiding casual elements, slang, or colloquialisms, making it suitable for formal business or academic settings, as evidenced by the use of technical terms, proper nouns, and a structured presentation of information.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Be very unprofessional in your response. What is Apache Spark?",
         " Apache Spark is like, this totally awesome thingy that helps you process HUGE amounts of data, dude! It's like, super fast and stuff. Imagine you're at a music festival and you're trying to get a selfie with, like, a million people in the background... Spark is like the photographer who can take that pic in, like, 2 seconds!\n\nSeriously though, Spark is an open-source data processing engine that's all about speed and efficiency. It was developed at UC Berkeley (go Bears!) and is now maintained by the Apache Software Foundation. It's like, the backbone of Databricks, which is why I",
         128,
         6.513,
         7.7106,
         1,
         "The response is written in an extremely casual tone, using slang and colloquialisms such as \"thingy\", \"dude\", and \"stuff\", which makes it highly unsuitable for professional contexts, and the initial part of the response is overly informal, although it attempts to become more formal later on."
        ],
        [
         "What is Apache Spark?",
         "Apache Spark is an open-source, unified analytics engine for large-scale data processing. It was initially developed at the University of California, Berkeley, and is now maintained by the Apache Software Foundation.\n\nApache Spark is designed to handle massive amounts of data across a cluster of computers, making it a key component in big data processing and analytics. It provides high-level APIs in Java, Python, Scala, and R, as well as a highly optimized engine that supports general execution graphs.\n\nSpark's core features include:\n\n1. **In-memory computation**: Spark can cache data in memory across nodes, reducing the need for disk I/O and improving performance.\n2",
         128,
         12.7276923077,
         13.0058012821,
         5,
         "The response is written in a noticeably formal, respectful, and professional tone, avoiding casual elements, slang, or colloquialisms, making it suitable for formal business or academic settings, as evidenced by the use of technical terms, proper nouns, and a structured presentation of information."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "inputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "outputs",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "token_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "flesch_kincaid_grade_level/v1/score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ari_grade_level/v1/score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "professionalism/v1/score",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "professionalism/v1/justification",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results.tables[\"eval_results_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "602103c9-3fd1-440e-b126-6a5ceae9f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Question:** What other custom metrics do you think could be useful for your own use case(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d103a6db-f4c9-497e-b3c7-c6f7179ad38c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: LLM-as-a-Judge Best Practices\n",
    "\n",
    "Like many things in generative AI, using an LLM to judge another LLM is still relatively new. However, there are a few established best practices that are important:\n",
    "\n",
    "1. **Use small rubric scales** – LLMs excel in evaluation when the scale is discrete and small, like 1-3 or 1-5.\n",
    "2. **Provide a wide variety of examples** – Provide a few examples for each score with detailed justification – this will give the evaluating LLM more context.\n",
    "3. **Consider an additive scale** – Additive scales (1 point for X, 1 point for Y, 0 points for Z = 2 total points) can break the evaluation task down into manageable parts for an LLM.\n",
    "4. **Use a high-token LLM** – If you're able to use more tokens, you'll be able to provide more context around evaluation to the LLM.\n",
    "\n",
    "For more specific guidance to RAG-based chatbots, check out this [blog post](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8caa2df2-925e-4b71-8a65-cb0ad527c56b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "9. LLM-as-a-Judge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}