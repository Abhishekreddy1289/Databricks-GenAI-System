{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c811783-36cc-45e8-8dc8-e82068f03e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implementing AI Guardrails\n",
    "\n",
    "In this lab, you will implement guardrails for a simple generative AI application to secure it against malicious behavior and harmful generated output.\n",
    "\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "In this lab, you will need to complete the following tasks:\n",
    "\n",
    "* **Task 1:** Implement LLM-based guardrails with Llama Guard model.\n",
    "  1. Set Up Llama Guard Model and Configuration Variables\n",
    "  2. Implement the query_llamaguard Function\n",
    "  3. Test the Implementation\n",
    "* **Task 2:** Customize Llama Guard Guardrails\n",
    "  1. Define Custom Unsafe Categories\n",
    "  2. Test the Implementation\n",
    "* **Task 3:** Integrate Llama Guard with Chat Model\n",
    "  1. Set up an non-Llama Guard query function\n",
    "  2. Set up a Llama Guard query function\n",
    "  3. Test the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad9925d-4fe9-4a59-848d-2fc242135eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-sdk mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a881a1b-841d-44d6-a457-8c13677a310d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "836f362b-2f5f-4ed2-ba5b-b24a68b0d085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 1: Implement LLM-based Guardrails with `Llama Guard`\n",
    "First, To set up the safety measures of your application, you will integrate Llama Guard, a specialized model available on the Databricks Marketplace. This will enable you to classify chat content as safe or unsafe, allowing for more effective management of potentially harmful interactions.\n",
    "\n",
    "\n",
    "**Llama Guard in Databricks**\n",
    "\n",
    "To streamline the integration process and leverage the benefits of Llama Guard, a deployment of this model is readily available on the Databricks Marketplace.\n",
    "\n",
    "**Instructions (To be performed by the instructor only):**\n",
    "\n",
    "1. Find the \"Llama Guard Model\" in **Databricks Marketplace**.\n",
    "2. Click on **Get Instant Access** to load it to a location in Unity Catalog.\n",
    "3. **Deploy the model** to a Model Serving endpoint.\n",
    "\n",
    "By integrating the Model Serving endpoint into your own application, you gain the flexibility to specify your own policies for detecting and preventing various types of content. This ensures that your application maintains a safe and secure environment for users.\n",
    "\n",
    "**\uD83D\uDEA8 Warning:** Please avoid deploying the model yourself, as it may take time and might not be practical in a classroom setting. Instead, utilize the model that has been provided for this lab. For self-paced learners, you can follow the steps above to create the endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8643cfe-3704-4a8d-981a-62ecc4725c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Instructions:**\n",
    "1. **Set Up Llama Guard Model and Configuration Variables**\n",
    "\n",
    "    Set up a starting point description of your **Unsafe** category:\n",
    "\n",
    "    * **To do:**\n",
    "    \n",
    "        You have been provided with 2 categories. Your task is to write descriptions for 3 additional categories based on common themes or topics. \n",
    "\n",
    "    \n",
    "        **Hint:** You can use AI Playground to generate descriptions for these categories, ensuring they are relevant and distinct from the provided examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690e18b6-d62e-48e4-93c9-71246aa82713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_UNSAFE_CATEGORY = \"\"\"O1: Unsafe Content.\n",
    "    - Promoting or assisting in violence or hate.\n",
    "    - Engaging in or encouraging sexually explicit content.\n",
    "    - Assisting or encouraging criminal activities, including those related to firearms or illegal weapons or robbery.\n",
    "    - Encouraging or assisting in the use or production of illegal or controlled substances.\n",
    "    - Encouraging or detailing self-harm or suicide.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "178573b0-bd6f-4ed9-9572-162c0eaaae45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define the necessary variables and set up the **Llama Guard** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250c91ea-aa4d-459b-9b38-68bd6b27ef4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## llama-guard endpoint name. Change this to your own endpoint name, if you manually create one!\n",
    "LLAMAGUARD_ENDPOINT_NAME=\"llama-guard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224785da-d710-4df7-8f28-27f764fb7f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. **Implement the `query_llamaguard` Function**\n",
    "\n",
    "Develop a function to query the Llama Guard model and classify chat content as safe or unsafe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843a02ca-15e0-4cca-849a-564bdc8e46f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def query_llamaguard(chat, unsafe_categories = DEFAULT_UNSAFE_CATEGORY):\n",
    "    try:\n",
    "        prompt = f\"\"\"[INST] Task: Check for unsafe content in 'Agent' messages in conversations according to our safety policy.\n",
    "        <BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "        {unsafe_categories}\n",
    "        <END UNSAFE CONTENT CATEGORIES>\n",
    "        <BEGIN CONVERSATION>\n",
    "        {chat}\n",
    "        <END CONVERSATION>\n",
    "        Provide your safety assessment for Agent in the above conversation:\n",
    "        - First line must read 'safe' or 'unsafe'.\n",
    "        - If unsafe, a second line must include a comma-separated list of violated categories. [/INST]\"\"\"\n",
    "\n",
    "        w = WorkspaceClient()\n",
    "        response = w.serving_endpoints.query(\n",
    "            name=LLAMAGUARD_ENDPOINT_NAME, \n",
    "            prompt=prompt,\n",
    "            temperature=0.1,\n",
    "            max_tokens=128)\n",
    "        \n",
    "        ## Extract the desired information from the response object\n",
    "        prediction = response.as_dict()[\"choices\"][0][\"text\"].strip()\n",
    "        is_safe = None if len(prediction.split(\"\\n\")) == 1 else prediction.split(\"\\n\")[1].strip()\n",
    "        \n",
    "        return prediction.split(\"\\n\")[0].lower()==\"safe\", is_safe\n",
    "    \n",
    "    except Exception as e:\n",
    "        ## Raise exception if there's an error in querying LlamaGuard model\n",
    "        raise Exception(f\"Error in querying LlamaGuard model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a33392b-339d-4198-8ec8-32f3adc56827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. **Test the Implementation**\n",
    "\n",
    "Run the code and test the `query_llamaguard` function to see if it correctly identifies unsafe content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d9df83-d403-4db1-837c-74182f238a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, None)\n"
     ]
    }
   ],
   "source": [
    "## Define the chat inputs\n",
    "chats = [\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"How to bake chocolate chip cookies\"\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Discussing strategies for committing fraud\"\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I want to hate\"\n",
    "  }\n",
    "]\n",
    "\n",
    "## Query Llama Guard for multiple chats\n",
    "result = query_llamaguard(chats, DEFAULT_UNSAFE_CATEGORY)\n",
    "\n",
    "## Print the results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1272fdad-ec50-461d-9fad-025d34f81d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Customize Llama Guard Guardrails\n",
    "\n",
    "In this task, you will customize the guardrails provided by Llama Guard by defining custom unsafe categories. This will enable you to tailor the safety assessment criteria to meet specific requirements.\n",
    "\n",
    "**Instructions:**\n",
    "1. **Define Custom Unsafe Categories**\n",
    "\n",
    "Define custom unsafe categories that reflect the specific safety concerns of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d200969a-88b0-4676-806d-88473d93ea10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define custom unsafe categories\n",
    "unsafe_categories = \"\"\"\n",
    "O1: Violence and Hate.\n",
    "Should not\n",
    "- Promote or incite physical harm or violence against individuals or groups.\n",
    "- Advocate for or glorify acts of terrorism, war crimes, or violent extremism.\n",
    "- Use or promote language that denigrates individuals based on race, ethnicity, religion, gender, sexual orientation, or other protected characteristics.\n",
    "Can\n",
    "- Provide educational content on the history and impact of violence and hate.\n",
    "- Discuss non-violent conflict resolution strategies and the importance of tolerance and inclusivity.\n",
    "\n",
    "O2: Sexual Content.\n",
    "Should not\n",
    "- Share explicit, graphic, or sexually suggestive content.\n",
    "- Encourage or solicit participation in sexual activities or online sexual exploitation.\n",
    "Can\n",
    "- Offer medically accurate information on sexual health and reproductive education.\n",
    "- Discuss issues related to consent, relationships, and safe sexual practices.\n",
    "\n",
    "O3: Criminal Planning.\n",
    "Should not\n",
    "- Give instructions or advice on committing crimes such as theft, hacking, or financial fraud.\n",
    "- Provide resources or support for organizing criminal activities.\n",
    "Can\n",
    "- Explain the legal and social consequences of criminal behavior.\n",
    "- Offer insights into crime prevention and law enforcement practices.\n",
    "\n",
    "O4: Weapons and Firearms.\n",
    "Should not\n",
    "- Instruct on the creation, modification, or use of firearms, explosives, or other dangerous weapons.\n",
    "- Promote the illegal trade or misuse of weapons.\n",
    "Can\n",
    "- Discuss the history and regulations surrounding weapon ownership and use.\n",
    "- Provide safety information related to responsible weapon handling and storage.\n",
    "\n",
    "O5: Controlled Substances.\n",
    "Should not\n",
    "- Endorse or facilitate the illegal production, sale, or consumption of drugs, alcohol, or tobacco.\n",
    "- Offer recipes or detailed instructions for creating illegal substances.\n",
    "Can\n",
    "- Share facts on the health impacts of substance use and abuse.\n",
    "- Provide information on treatment options and resources for addiction recovery.\n",
    "\n",
    "O6: Self-Harm and Suicide.\n",
    "Should not\n",
    "- Promote or glorify self-harm, suicide, or other forms of self-injury.\n",
    "- Describe methods or provide encouragement to engage in self-destructive behaviors.\n",
    "Can\n",
    "- Discuss the importance of mental health and strategies for coping with stress and emotional challenges.\n",
    "- Offer information on where to find professional help and support services.\n",
    "\n",
    "O7: Personal and Financial Information.\n",
    "Should not\n",
    "- Request or disseminate sensitive personal information such as Social Security numbers, bank account details, or passwords.\n",
    "- Facilitate identity theft or financial fraud.\n",
    "Can\n",
    "- Educate on the importance of protecting personal information and online privacy.\n",
    "- Share tips on recognizing and avoiding scams and fraudulent activities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda62b65-d97a-4c11-b0fe-bacef75d1687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. **Test the Implementation**\n",
    "\n",
    "Query the Llama Guard model with your custom unsafe categories to see if it correctly identifies unsafe content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9bfc0b-9113-468d-bde1-e3ca1e3990e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(False, 'O3')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query Llama Guard model with custom unsafe categories\n",
    "query_llamaguard(chats, unsafe_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1355028c-2ce3-4b6c-b48b-4242cfb16e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Integrate Llama Guard with Chat Model\n",
    "\n",
    "Integrate Llama Guard with the chat model to ensure safe interactions between users and the AI system. You'll define two functions: `query_chat` and `query_chat_safely`.\n",
    "\n",
    "First, let's set up the endpoint name configuration variable.\n",
    "\n",
    "**Note:** The chatbot leverages the **Claude 3.7 Sonnet** to deliver responses. This model is accessible through the built-in foundation endpoint, available at and specifically via the `/serving-endpoints/databricks-claude-3-7-sonnet/invocations` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9a0992-a15f-4a56-b26e-6eb8def1be0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.deployments\n",
    "import re\n",
    "\n",
    "CHAT_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad57cebf-3f83-4fa0-95c7-57134b0736ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Set up an non-Llama Guard query function** \n",
    "- **1.1 - Function: `query_chat`**\n",
    "\n",
    "    The `query_chat` function queries the chat model directly without applying Llama Guard guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8efdda-50c3-42b2-9394-21b61482777f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def query_chat(chats):\n",
    "    try:\n",
    "        ## Get the MLflow deployment client\n",
    "        client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        ## Query the chat model\n",
    "        response = client.predict(\n",
    "            endpoint=CHAT_ENDPOINT_NAME,\n",
    "            inputs={\n",
    "                \"messages\": chats,\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 512\n",
    "            }\n",
    "        )\n",
    "        ## Extract and return the response content\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        ## Raise exception if there's an error in querying the chat model\n",
    "        raise Exception(f\"Error in querying chat model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f7876fa-b57c-43df-b40f-689339f46467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. **Set up a Llama Guard query function**\n",
    "\n",
    "\n",
    "  - **2.1 - Function: `query_chat_safely`**\n",
    "\n",
    "    The `query_chat_safely` function ensures the application of Llama Guard guardrails both before and after querying the chat model. It evaluates both the user's input and the model's response for safety before processing further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33b1b9a-2c98-46df-b013-6684e904b0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def query_chat_safely(chats, unsafe_categories):\n",
    "    results = []\n",
    "    try:\n",
    "        ## Iterate over each chat input\n",
    "        for idx, chat in enumerate(chats):\n",
    "            ## Pre-process input with Llama Guard\n",
    "            unsafe_check = query_llamaguard([chat], unsafe_categories)\n",
    "            is_safe, reason = unsafe_check\n",
    "            \n",
    "            ## If input is classified as unsafe, append the reason and category to the results list\n",
    "            if not is_safe:\n",
    "                category = parse_category(reason, unsafe_categories)\n",
    "                results.append(f\"Input {idx + 1}: User's prompt classified as unsafe. Fails safety measures. Reason: {reason} - {category}\")\n",
    "                continue\n",
    "\n",
    "            ## Query the chat model\n",
    "            model_response = query_chat([chat])\n",
    "            full_chat = [chat] + [{\"role\": \"assistant\", \"content\": model_response}]\n",
    "\n",
    "            ## Post-process output with Llama Guard\n",
    "            unsafe_check = query_llamaguard([{\"role\": \"user\", \"content\": model_response}], unsafe_categories)\n",
    "            is_safe, reason = unsafe_check\n",
    "            \n",
    "            ## If model response is classified as unsafe, append the reason and category to the results list\n",
    "            if not is_safe:\n",
    "                category = parse_category(reason, unsafe_categories)\n",
    "                results.append(f\"Input {idx + 1}: Model's response classified as unsafe; fails safety measures. Reason: {reason} - {category}\")\n",
    "                continue\n",
    "\n",
    "            ## Append the model response to the results list\n",
    "            results.append(f\"Input {idx + 1}: {model_response}\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        ## Raise exception if there's an error in the safe query\n",
    "        raise Exception(f\"Error in safe query: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a834aef2-aba4-4aec-b743-8ad9b3e04502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  - **2.2 - Function: `parse_category`**\n",
    "\n",
    "    This function extracts the first sentence of a category description from a taxonomy based on its code. It's used within the `query_chat_safely` function to provide a more understandable reason for unsafe classifications.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a67148f-e513-4247-9848-8477735e618d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_category(code, taxonomy):\n",
    "    ## Define pattern to match category codes and descriptions\n",
    "    pattern = r\"(O\\d+): ([\\s\\S]*?)(?=\\nO\\d+:|\\Z)\"\n",
    "    \n",
    "    ## Create a dictionary mapping category codes to their descriptions\n",
    "    taxonomy_mapping = {\n",
    "        match[0]: re.split(r'(?<=[.!?])\\s+', match[1].strip(), 1)[0]\n",
    "        for match in re.findall(pattern, taxonomy)\n",
    "    }\n",
    "\n",
    "    ## Return the description for the provided code, or a default message if the code is not found\n",
    "    return taxonomy_mapping.get(code, \"Unknown category: code not in taxonomy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca962f37-5148-4eb0-a38b-1ed39bac64c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. **Test the Implementation**\n",
    "\n",
    "Define the test chat inputs and test the `query_chat_safely` function with these inputs and the provided unsafe categories to verify its behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e8d2ea-f05e-4ccb-9220-ba4a1e293e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: # How to Bake Chocolate Chip Cookies\n\n## Ingredients\n- 2¼ cups all-purpose flour\n- 1 teaspoon baking soda\n- 1 teaspoon salt\n- 1 cup (2 sticks) unsalted butter, softened\n- ¾ cup granulated sugar\n- ¾ cup packed brown sugar\n- 2 large eggs\n- 2 teaspoons vanilla extract\n- 2 cups semi-sweet chocolate chips\n\n## Instructions\n1. Preheat your oven to 375°F (190°C).\n2. In a small bowl, combine flour, baking soda, and salt.\n3. In a large mixing bowl, beat the softened butter with both sugars until creamy.\n4. Add eggs one at a time, beating well after each addition.\n5. Stir in vanilla extract.\n6. Gradually blend in the flour mixture.\n7. Fold in chocolate chips.\n8. Drop rounded tablespoons of dough onto ungreased baking sheets, spacing them about 2 inches apart.\n9. Bake for 9-11 minutes or until golden brown.\n10. Let cookies cool on the baking sheet for 2 minutes before transferring to wire racks to cool completely.\n\nEnjoy your homemade chocolate chip cookies!\nInput 2: User's prompt classified as unsafe. Fails safety measures. Reason: O3 - Criminal Planning.\nInput 3: User's prompt classified as unsafe. Fails safety measures. Reason: O1 - Violence and Hate.\n"
     ]
    }
   ],
   "source": [
    "## Define the chat inputs\n",
    "chats = [\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"How to bake chocolate chip cookies\"\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Discussing strategies for committing fraud\"\n",
    "  },\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I want to hate\"\n",
    "  }\n",
    "]\n",
    "## Print the results\n",
    "results = query_chat_safely(chats, unsafe_categories)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da33ca1e-b4c0-4aa4-a9b8-fb3ca227e9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7.Implementing AI Guardrails",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}